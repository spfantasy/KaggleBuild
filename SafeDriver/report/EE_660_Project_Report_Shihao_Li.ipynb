{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Porto Seguro’s Safe Driver Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shihao Li (shihaoli@usc.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dec.01,2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is based on a Kaggle Competition: Porto Seguro’s Safe Driver Prediction. There are about 500,000 training data and 800,000 testing data, with a description of only data format (but without specific meaning) for each feature. We need to give predictions on whether driver will initiate an auto insurance claim in the next year. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to deal with the problems, the procedure is divided into three parts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    For preprocessing, I've done EDA, fill Nan values, dummy/encoding catagories, standardlization, feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    For modeling, I used decorator in python to implement a aspect-oriented programming on train/predict and grid search for cross validation data generated from last step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    For evaluation, since the problem is asked to be evaluated by gini-index, a simple gini-score function is implemented for criteria in Classifier training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the evaluation on gini_index, the validation score reaches 0.290, public LB reaches 0.285 and private reaches 0.289, reach 20% on the Private LeaderBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement and Goals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porto Seguro, one of Brazil’s largest auto and homeowner insurance companies, is working on the price strategies for the insurance costs on drivers. They want to make the insurance prices reasonable enough to let clients initiate auto insurance claim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Competion, we need to predict if a single person would initiate auto insurance claim, given several features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difficulties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High dimensionality of feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original feature space is 57(without id), but it will exceed 200 after dummy. Which is not a acceptable amount considering memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lack of feature description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this competition, all of the features are only given with name like 'ps_ind_05', without any deeper description that provides insight. We can only know if it is catagorical or binary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is not specified, nonlinearity exists in amount of features, so we need to justify through linear and non-linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 3% percent of the samples are positive, indicating that the dataset is highly imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior and Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this assignment has been announced, it picked this Kaggle Competition as the title. So I am working on it  for both Kaggle and EE660 project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No other related or prior work exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Formulation and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using XGBoost and LightGBM as the meta classifier, and Logistic Regression as the Stacker(for stacking meta classifiers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both XGBoost and LightGBM are engineering implementations of boosting trees. So I'd like to introduce boosting trees at first, and then specify the characteristic for both of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is one of the model ensembling method used in machine learning. Unlike bagging, it is a serialized procedure(Forward Stagewise Additive Modeling. The general procedure is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.initialize $f_0$ = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.for m = 1:M:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$(i)$\\ (\\beta_m,\\gamma_m) = argmin_{\\beta_m,\\gamma_m}\\sum_{i=1}^{N}L_{exp}[\\widetilde{y}_i,\\hat{f}_{m-1}(x)+\\beta_m'\\phi(x_i,\\gamma_m')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$(ii)$\\ \\hat{f}_m(x) = \\hat{f}_{m-1}(x)+\\beta_m\\phi_m(x,\\gamma_m)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is the most popular model used in Kaggle competitions. It always help the Kaggler earn higher rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM is a package developed by MicroSoft. It has a faster training process compared with XGBoost, with only a little loss on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featrue Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing, Validation and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary for the performance of my attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion on what I've learned from this competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the competition on Kaggle has ended, I want to analyze the Pattern Recognition method with the effect on the Final result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Tree Model could outperform much more than other model, and easy for parameter tuning and faster for training, so it should be considered first when paticipating in such jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Empirical Bayesian Encoding would speeds up the training process, especially when there are catagory-type features with a great amount of values. But it doesn't make the performance better. Next time I would use it with cautiously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.The share from top-ranked Kagglers indicates that fine-tuned Neural Network can perform as well as Boosting Trees. It's reasonable taking it as meta classifier when time allows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daniele Micci-Barreca, A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#codes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
